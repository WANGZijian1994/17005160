{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TP 2 word-embeddings",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WANGZijian1994/17005160/blob/master/TP_2_word_embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fKDWGQlT_hP"
      },
      "source": [
        "Copy this notebook (File>Save a copy in Drive) and then work on your copy.\n",
        "==\n",
        "To send me your work: Share(top-right of the window)>Get link/copy link\n",
        "\n",
        "Send me an email containing the link after having\n",
        "*   changed the link permission to Editor and\n",
        "*   allowed anyone with the link to access the notebook.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2BZykSRWlSN"
      },
      "source": [
        "Goal\n",
        "==\n",
        "\n",
        "We are about to design and train a neural system to perform sentiment analysis on film reviews. More precisely, the network will have to output the probability that the input review expresses a positive opinion (overall).\n",
        "\n",
        "The system will be a bag-of-words model using GloVe embeddings. It will have to first average the embeddings of the words of the input review, and then send the result through a simple network that should output a probability.\n",
        "\n",
        "There is a lot of already written code at the beginning of the notebook. It is important that you understand it as you will have to reuse/reproduce it for future work."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YowcYrkUOwG"
      },
      "source": [
        "Loading Pytorch is important.\n",
        "=="
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2dazbs4RArm"
      },
      "source": [
        "# Imports Pytorch.\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEOouXSvWiNA"
      },
      "source": [
        "Downloading the dataset\n",
        "==\n",
        "The dataset we are going to use this the Large Movie Review Dataset (https://ai.stanford.edu/~amaas/data/sentiment/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pD8AIWygRhI5"
      },
      "source": [
        "# Downloads the dataset.\n",
        "import urllib\n",
        "\n",
        "tmp = urllib.request.urlretrieve(\"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\")\n",
        "filename = tmp[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXNSehWtRsXE"
      },
      "source": [
        "filename"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "097EOlPhS07G"
      },
      "source": [
        "# Extracts the dataset.\n",
        "import tarfile\n",
        "tar = tarfile.open(filename)\n",
        "tar.extractall()\n",
        "tar.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sp3XCqJ5TS73"
      },
      "source": [
        "import os # Useful library to read files and inspect directories."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOruARLhTU4e"
      },
      "source": [
        "# Shows which files and directories are present at the root of the file system.\n",
        "for filename in os.listdir(\".\"):\n",
        "  print(filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqYr8dYnRtCx"
      },
      "source": [
        "dataset_root = \"aclImdb\"\n",
        "# Shows which files and directories are present at the root of the dataset directory.\n",
        "for filename in os.listdir(dataset_root):\n",
        "  print(filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VM9B2NreR-MT"
      },
      "source": [
        "# Shows several reviews.\n",
        "dirname = os.path.join(dataset_root, \"train\", \"neg\") # \"aclImdb/{train|test}/{neg|pos}\"\n",
        "for idx, filename in enumerate(os.listdir(dirname)):\n",
        "  if(idx >= 5): break # Stops after the 5th file.\n",
        "  \n",
        "  print(filename)\n",
        "  with open(os.path.join(dirname, filename)) as f:\n",
        "    review = f.read()\n",
        "    print(review)\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikcb0jJiaotG"
      },
      "source": [
        "Preprocessing the dataset\n",
        "=="
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfsLxzRGctTt"
      },
      "source": [
        "import nltk # Imports NLTK, an NLP library.\n",
        "nltk.download('punkt') # Loads a module required for tokenization.\n",
        "import collections # This library defines useful data structures. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NWWSBBYUoto"
      },
      "source": [
        "newline = \"<br />\" # The reviews sometimes contain this HTLM tag to indicate a line break.\n",
        "def preprocess(text):\n",
        "  text = text.replace(newline, \" \") # Replaces the newline HTML tag with a space.\n",
        "  tokens = nltk.word_tokenize(text); # List of tokens (strings).\n",
        "  tokens = [token.lower() for token in tokens] # Lowercases all tokens.\n",
        "  \n",
        "  return tokens\n",
        "\n",
        "# Reads and pre-processes the reviews.\n",
        "dataset = {\"train\": [], \"test\": []}\n",
        "binary_classes = {\"neg\": 0, \"pos\": 1}\n",
        "for part_name, l in dataset.items():\n",
        "  for class_name, value in binary_classes.items():\n",
        "    path = os.path.join(dataset_root, part_name, class_name)\n",
        "    print(\"Processing %s...\" % path, end='');\n",
        "    for filename in os.listdir(path):\n",
        "        with open(os.path.join(path, filename)) as f:\n",
        "          review_text = f.read()\n",
        "          review_tokens = preprocess(review_text)\n",
        "          \n",
        "          l.append((review_tokens, value))\n",
        "    print(\" done\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlDG9piYiVHL"
      },
      "source": [
        "# Splits the train set into a proper train set and a development/validation set.\n",
        "# 'dataset[\"train\"]' happens to be a list composed of a certain number of negative examples followed by the same number of positive examples.\n",
        "# We are going to use 3/4 of the original train set as our actual train set, and 1/4 as our development set.\n",
        "# We want to keep balanced train and development sets, i.e. for both, half of the reviews should be positive and half should be negative.\n",
        "if(\"dev\" in dataset): print(\"This should only be run once.\")\n",
        "else:\n",
        "  dev_set_half_size = int((len(dataset[\"train\"]) / 4) / 2) # Half of a quarter of the training set size.\n",
        "  dataset[\"dev\"] = dataset[\"train\"][:dev_set_half_size] + dataset[\"train\"][-dev_set_half_size:] # Takes some negative examples at the beginning and some positive ones at the end.\n",
        "  dataset[\"train\"] = dataset[\"train\"][dev_set_half_size:-dev_set_half_size] # Removes the examples used for the development set.\n",
        "\n",
        "  for (part, data) in dataset.items():\n",
        "    class_counts = collections.defaultdict(int)\n",
        "    for (_, p) in data: class_counts[p] += 1\n",
        "    print(f\"{part}: {class_counts}\")\n",
        "  print(\"Train set split into train/dev.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WfTvFxXvjvr"
      },
      "source": [
        "for x in dataset:\n",
        "  print(x,\" \",len(dataset[x]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfdibF5dhMIh"
      },
      "source": [
        "Loading the word embeddings\n",
        "==\n",
        "We are going to use GloVe embeddings.\n",
        "\n",
        "All word forms with a frequency below a given threshold are going to be considered unknown forms."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnkKPGLYxQNR"
      },
      "source": [
        "# Computes the frequency of all word forms in the train set.\n",
        "word_counts = collections.defaultdict(int)\n",
        "for tokens, _ in dataset[\"train\"]:\n",
        "  for token in tokens: word_counts[token] += 1\n",
        "\n",
        "print(word_counts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xgw19ofeZ23K"
      },
      "source": [
        "# Builds a vocabulary containing only those words present in the train set with a frequency above a given threshold.\n",
        "count_threshold = 4;\n",
        "vocabulary = set()\n",
        "for word, count in word_counts.items():\n",
        "    if(count > count_threshold): vocabulary.add(word)\n",
        "\n",
        "print(vocabulary)\n",
        "print(len(vocabulary))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mi6oZU1kph03"
      },
      "source": [
        "import zipfile\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnhHpcuxckbK"
      },
      "source": [
        "# Returns a dictionary {word[String]: id[Integer]} and a list of Numpy arrays\n",
        "# `data_path` is the path of the directory containing the GloVe files (if None, 'glove.6B' is used)\n",
        "# `max_size` is the number of word embeddings read (starting from the most frequent; in the GloVe files, the words are sorted)\n",
        "# If `vocabulary` is specified, the output vocabulary contains the intersection of `vocabulary` and the words with a defined embedding. Otherwise, all words with a defined embedding are used.\n",
        "def get_glove(dim=50, vocabulary=None, max_size=-1, data_path=None):\n",
        "  dimensions = set([50, 100, 200, 300]) # Available dimensions for GloVe 6B\n",
        "  fallback_url = 'http://nlp.stanford.edu/data/glove.6B.zip' # (Remember that in GloVe 6B, words are lowercased.)\n",
        "\n",
        "  assert (dim in dimensions), (f'Unavailable GloVe 6B dimension: {dim}.')\n",
        "\n",
        "  if(data_path is None): data_path = 'glove.6B'\n",
        "\n",
        "  # Checks that the data is here, otherwise downloads it.\n",
        "  if(not os.path.isdir(data_path)):\n",
        "    #print('Directory \"%s\" does not exist. Creation.' % data_path)\n",
        "    os.makedirs(data_path)\n",
        "  \n",
        "  glove_weights_file_path = os.path.join(data_path, f'glove.6B.{dim}d.txt')\n",
        "  \n",
        "  if(not os.path.isfile(glove_weights_file_path)):\n",
        "    local_zip_file_path = os.path.join(data_path, os.path.basename(fallback_url))\n",
        "  \n",
        "    if(not os.path.isfile(local_zip_file_path)):\n",
        "      print(f'Retreiving GloVe embeddings from {fallback_url}.')\n",
        "      urllib.request.urlretrieve(fallback_url, local_zip_file_path)\n",
        "    \n",
        "    with zipfile.ZipFile(local_zip_file_path, 'r') as z:\n",
        "      print(f'Extracting GloVe embeddings from {local_zip_file_path}.')\n",
        "      z.extractall(path=data_path)\n",
        "  \n",
        "  assert os.path.isfile(glove_weights_file_path), (f\"GloVe file {glove_weights_file_path} not found.\")\n",
        "\n",
        "  # Reads GloVe data.\n",
        "  print('Reading GloVe embeddings.')\n",
        "  new_vocabulary = {} # A dictionary {word[String]: id[Integer]}\n",
        "  embeddings = [] # The list of embeddings (Numpy arrays)\n",
        "  with open(glove_weights_file_path, 'r') as f:\n",
        "    for line in f: # Each line consist of the word followed by a space and all of the coefficients of the vector separated by a space.\n",
        "      values = line.split()\n",
        "\n",
        "      # Here, I'm trying to detect where on the line the word ends and where the vector begins. As in some version(s) of GloVe words can contain spaces, this is not entirely trivial.\n",
        "      vector_part = ' '.join(values[-dim:])\n",
        "      x = line.find(vector_part)\n",
        "      word = line[:(x - 1)]\n",
        "\n",
        "      if((vocabulary is not None) and (not word in vocabulary)): # If a vocabulary was specified and if the word is not it…\n",
        "        continue # …this word is skipped.\n",
        "\n",
        "      new_vocabulary[word] = len(new_vocabulary)\n",
        "      embedding = np.asarray(values[-dim:], dtype=np.float32)\n",
        "      embeddings.append(embedding)\n",
        "\n",
        "      if(len(new_vocabulary) == max_size): break\n",
        "  print('(GloVe embeddings loaded.)')\n",
        "  print()\n",
        "\n",
        "  return (new_vocabulary, embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KY9nJNVlpZiL"
      },
      "source": [
        "(new_vocabulary, embeddings) = get_glove(dim=50, vocabulary=vocabulary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwaov0tWpeFZ"
      },
      "source": [
        "print(len(new_vocabulary)) # Shows the size of the vocabulary.\n",
        "print(new_vocabulary) # Shows each word and its id."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIY-HdiPhgL4"
      },
      "source": [
        "Batch generator\n",
        "=="
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x07xfo-Gux_X"
      },
      "source": [
        "# Defines a class of objects that produce batches from the dataset.\n",
        "class BatchGenerator:\n",
        "  def __init__(self, dataset, vocabulary):\n",
        "    self.dataset = dataset\n",
        "    for part in self.dataset.values(): # Shuffles the dataset so that positive and negative examples are mixed.\n",
        "      np.random.shuffle(part)\n",
        "\n",
        "    self.vocabulary = vocabulary # Dictonary {word[String]: id[Integer]}\n",
        "    self.unknown_word_id = len(vocabulary) # Id for unknown forms\n",
        "    self.padding_idx = len(vocabulary) + 1 # Not all reviews of a given batch will have the same length. We will \"pad\" shorter reviews with a special token id so that the batch can be represented by a matrix.\n",
        "  \n",
        "  def length(self, data_type='train'):\n",
        "    return len(self.dataset[data_type])\n",
        "\n",
        "  # Returns a random batch.\n",
        "  # If `subset` is an integer, only a subset of the corpus is used. This can be useful to debug the system.\n",
        "  def get_batch(self, batch_size, data_type, subset=None):\n",
        "    data = self.dataset[data_type] # selects the relevant portion of the dataset.\n",
        "    \n",
        "    max_i = len(data) if(subset is None) else min(subset, len(data))\n",
        "    instance_ids = np.random.randint(max_i, size=batch_size) # Randomly picks some instance ids.\n",
        "\n",
        "    return self._ids_to_batch(data, instance_ids)\n",
        "\n",
        "  def _ids_to_batch(self, data, instance_ids):\n",
        "    word_ids = [] # Will be a list of lists of word ids (Integer)\n",
        "    polarity = [] # Will be a list of review polarities (Boolean)\n",
        "    texts = [] # Will be a list of lists of words (String)\n",
        "    for instance_id in instance_ids:\n",
        "      text, p = data[instance_id]\n",
        "\n",
        "      word_ids.append([self.vocabulary.get(w, self.unknown_word_id) for w in text])\n",
        "      polarity.append(p)\n",
        "      texts.append(text)\n",
        "    \n",
        "    # Padding\n",
        "    self.pad(word_ids)\n",
        "\n",
        "    word_ids = torch.tensor(word_ids, dtype=torch.long) # Conversion to a tensor\n",
        "    polarity = torch.tensor(polarity, dtype=torch.bool) # Conversion to a tensor\n",
        "\n",
        "    return (word_ids, polarity, texts) # We don't really need `texts` but it might be useful to debug the system.\n",
        "  \n",
        "  # Pads a list of lists (i.e. adds fake word ids so that all sequences in the batch have the same length, so that we can use a matrix to represent them).\n",
        "  # In place\n",
        "  def pad(self, word_ids):\n",
        "    max_length = max([len(s) for s in word_ids])\n",
        "    for s in word_ids: s.extend([self.padding_idx] * (max_length - len(s)))\n",
        "  \n",
        "  # Returns a generator of batches for a full epoch.\n",
        "  # If `subset` is an integer, only a subset of the corpus is used. This can be useful to debug the system.\n",
        "  def all_batches(self, batch_size, data_type=\"train\", subset=None):\n",
        "    data = self.dataset[data_type]\n",
        "    \n",
        "    max_i = len(data) if(subset is None) else min(subset, len(data))\n",
        "\n",
        "    # Loop that generates all full batches (batches of size 'batch_size')\n",
        "    i = 0\n",
        "    while((i + batch_size) <= max_i):\n",
        "      instance_ids = np.arange(i, (i + batch_size))\n",
        "      yield self._ids_to_batch(data, instance_ids)\n",
        "      i += batch_size\n",
        "    \n",
        "    # Possibly generates the last (not full) batch.\n",
        "    if(i < max_i):\n",
        "      instance_ids = np.arange(i, max_i)\n",
        "      yield self._ids_to_batch(data, instance_ids)\n",
        "  \n",
        "  # Turns a list of arbitrary pre-processed texts into a batch.\n",
        "  # This function will be used to infer the polarity of a unannotated review.\n",
        "  def turn_into_batch(self, texts):\n",
        "    word_ids = [[self.vocabulary.get(w, self.unknown_word_id) for w in text] for text in texts]\n",
        "    self.pad(word_ids)\n",
        "    return torch.tensor(word_ids, dtype=torch.long)\n",
        "\n",
        "batch_generator = BatchGenerator(dataset=dataset, vocabulary=new_vocabulary)\n",
        "print(batch_generator.length('train')) # Prints the number of instance in the train set."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v35rEb8l0_Kd"
      },
      "source": [
        "tmp = batch_generator.get_batch(3, data_type=\"train\")\n",
        "print(tmp[0]) # Prints the matrix of token ids.\n",
        "print(tmp[1]) # Prints the vector of polarities.\n",
        "print(tmp[2]) # Prints the list of reviews."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cp2VOTBzFLb9"
      },
      "source": [
        "len(list(batch_generator.all_batches(batch_size=3, data_type=\"train\"))) # Number of batches of size 3 in the training set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsTuIZoIhkTW"
      },
      "source": [
        "The model\n",
        "=="
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PE6mbJ9Q2nUy"
      },
      "source": [
        "class SentimentClassifier(torch.nn.Module):\n",
        "  # embeddings: list of Numpy arrays\n",
        "  # hidden_sizes: list of the size of all hidden layers (Integer)\n",
        "  def __init__(self, embeddings, hidden_sizes, freeze_embeddings=True, device='cpu'):\n",
        "    embeddings = list(embeddings) # Creates a copy of the list of embeddings, so we can add or remove entries without affecting the original list.\n",
        "    super().__init__() # Calls the constructor of the parent class. Usually, this is necessary when creating a custom module.\n",
        "\n",
        "    self.padding_idx = len(embeddings) + 1 # len(embeddings) will be the id of the embedding of the unknown word\n",
        "\n",
        "    # Here you have to (i) define a vector for unknown forms (the average of actual word embeddings) and a vector for the padding token (full of 0·s) and (ii) define an embedding layer 'self.embeddings' using torch.nn.Embedding and not forgeting to use the 'freeze' and 'padding_idx' arguments.\n",
        "    #################\n",
        " \n",
        "    #################\n",
        "    self.embeddings = self.embeddings.to(device) # Sends the word embeddings to 'device', which is potentially a GPU.\n",
        "\n",
        "    # Here you have to define self.main_part, the network that computes a probability out of a review (represented as the average of the embeddings of the tokens).\n",
        "    # The number of hidden layers is determined by 'hidden_sizes, which is a list of integers describing the (output) size of each of them.\n",
        "    # Use torch.nn.Linear to build linear layers.\n",
        "    # torch.nn.Sequential takes one argument per module and not a list of modules as argument, but if 'modules' is a list of modules, 'torch.nn.Sequential(*modules)' (with the star notation) works.\n",
        "    #################\n",
        "\n",
        "    #################\n",
        "    self.main_part = self.main_part.to(device) # Sends the network to 'device', which is potentially a GPU.\n",
        "\n",
        "    self.device = device\n",
        "\n",
        "  # 'batch' is a matrix of word ids (Integer).\n",
        "  def forward(self, batch):\n",
        "    # Here you have to (i) turns 'batch' into a matrix of embeddings (i.e. a tensor of rank 3), (ii) average all embeddings for a given review being careful not to take into account padding vectors, (iii) send these bag-of-words representations to the network.\n",
        "    # Return a tensor of shape (batch size) instead of (batch size, 1).\n",
        "    #################\n",
        " \n",
        "    #################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpqcxScW4Afb"
      },
      "source": [
        "model = SentimentClassifier(embeddings, hidden_sizes=[100], freeze_embeddings=True)\n",
        "batch = batch_generator.get_batch(3, data_type=\"train\")\n",
        "print(model(batch[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmL7aPacmNVh"
      },
      "source": [
        "# Function that computes the accuracy of the model on a given part of the dataset.\n",
        "evaluation_batch_size = 256\n",
        "def evaluation(data_type, subset=None):\n",
        "  nb_correct = 0\n",
        "  total = 0\n",
        "  for batch in batch_generator.all_batches(batch_size, data_type=data_type, subset=subset):\n",
        "    prob = model(batch[0].to(model.device)) # Forward pass\n",
        "    answer = (prob > 0.5) # Shape: (batch_size, 1)\n",
        "    nb_correct += (answer == batch[1].to(model.device)).sum().item()\n",
        "    total += batch[0].shape[0]\n",
        "      \n",
        "  accuracy = (nb_correct / total)\n",
        "  return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vs_gHaFzrOaj"
      },
      "source": [
        "Training\n",
        "==\n",
        "Once everything works, feel free to find better hyperparameters.\n",
        "The goal is to maximise the accuracy on the development set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOMCyVPK6BD8"
      },
      "source": [
        "model = SentimentClassifier(embeddings, hidden_sizes=[20,10], freeze_embeddings=False, device='cuda')\n",
        "\n",
        "# Tests the model on a couple of instance before training.\n",
        "model.eval() # Tells Pytorch we are in evaluation/inference mode (can be useful if dropout is used, for instance).\n",
        "print(model(batch_generator.turn_into_batch([preprocess(text) for text in [\"This movie was terrible!!\", \"Pure gold!\"]]).to(model.device)))\n",
        "\n",
        "# Training procedure\n",
        "learning_rate = 0.004\n",
        "l2_reg = 0.0001\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.99, weight_decay=l2_reg) # Once the backward propagation has been done, call the 'step' method (with no argument) to update the parameters.\n",
        "batch_size = 64\n",
        "subset = None # Use an integer to train on a smaller portion of the training set, otherwise use None.\n",
        "epoch_size = batch_generator.length(\"train\") if(subset is None) else subset # In number of instances\n",
        "\n",
        "nb_epoch = 20\n",
        "epoch_id = 0 # Id of the current epoch\n",
        "instances_processed = 0 # Number of instances trained on in the current epoch\n",
        "epoch_loss = [] # Will contain the loss for each batch of the current epoch\n",
        "while(epoch_id < nb_epoch):\n",
        "  model.train() # Tells Pytorch we are in training mode (can be useful if dropout is used, for instance).\n",
        "  \n",
        "  model.zero_grad() # Makes sure the gradient is reinitialised to zero.\n",
        "  \n",
        "  batch = batch_generator.get_batch(batch_size, data_type=\"train\", subset=subset)\n",
        "\n",
        "  # You have to (i) compute the prediction of the model, (ii) compute the loss (use an average over the batch), (iii) call \"backward\" on the loss and (iv) store the loss in \"epoch_loss\".\n",
        "  ###################\n",
        "\n",
        "  ###################\n",
        "  \n",
        "  optimizer.step() # Updates the parameters.\n",
        "\n",
        "  instances_processed += batch_size\n",
        "  if(instances_processed > epoch_size):\n",
        "    print(f\"-- END OF EPOCH {epoch_id}.\")\n",
        "    print(f\"Average loss: {sum(epoch_loss) / len(epoch_loss)}.\")\n",
        "\n",
        "    # Evaluation\n",
        "    model.eval() # Tells Pytorch we are in evaluation/inference mode (can be useful if dropout is used, for instance).\n",
        "    with torch.no_grad(): # Deactivates Autograd (it is computationaly expensive and we don't need it here).\n",
        "      accuracy = evaluation(\"train\")\n",
        "      print(f\"Accuracy on the train set: {accuracy}.\")\n",
        "\n",
        "      accuracy = evaluation(\"dev\")\n",
        "      print(f\"Accuracy on the dev set: {accuracy}.\")\n",
        "\n",
        "    epoch_id += 1\n",
        "    instances_processed -= epoch_size\n",
        "    epoch_loss = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zh4na1hivgXP"
      },
      "source": [
        "model.eval() # Tells Pytorch we are in evaluation/inference mode (can be useful if dropout is used, for instance).\n",
        "model(batch_generator.turn_into_batch([preprocess(text) for text in [\"This movie was terrible!!\", \"Pure gold!\", \"Bad.\", \"Not bad!\"]]).to(model.device))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CS6Wqo5Srxhi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}